---
author: Mathias-Axel Niato
publishDate: 2024-09-10T14:30:00Z
title: "Computer Vision temps réel avec Python et OpenCV"
tags:
  - computer-vision
  - python
  - opencv
  - temps-reel
description: "Guide pratique pour développer des applications de computer vision temps réel basé sur mon expérience chez FittingBox"
cover:
  src: './images/computer-vision-realtime/cover.webp'
  alt: 'Interface de computer vision temps réel'
---

## Introduction à la Computer Vision temps réel

La computer vision temps réel est au cœur de nombreuses applications modernes : reconnaissance faciale, réalité augmentée, robotique, surveillance automatique, etc. Dans le cadre de mon stage chez **FittingBox**, j'ai développé des outils Python pour la mesure de paramètres oculaires en temps réel. Voici un retour d'expérience sur les techniques et optimisations essentielles.

## Fondamentaux du traitement temps réel

### Contraintes temporelles
Pour une application temps réel, nous devons respecter des contraintes strictes :
- **Latence** : < 100ms pour une interaction fluide
- **Framerate** : Minimum 25 FPS (40ms par frame)
- **Jitter** : Variation temporelle minimale

### Architecture pipeline
```python
import cv2
import numpy as np
import time
from threading import Thread
from queue import Queue

class RealTimeVisionPipeline:
    def __init__(self):
        self.camera = cv2.VideoCapture(0)
        self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.camera.set(cv2.CAP_PROP_FPS, 30)
        
        self.frame_queue = Queue(maxsize=2)
        self.result_queue = Queue(maxsize=2)
        self.running = False
        
    def capture_frames(self):
        """Thread dédié à la capture d'images"""
        while self.running:
            ret, frame = self.camera.read()
            if ret:
                # Garder seulement la frame la plus récente
                if not self.frame_queue.full():
                    self.frame_queue.put(frame)
                else:
                    # Vider la queue et mettre la nouvelle frame
                    try:
                        self.frame_queue.get_nowait()
                    except:
                        pass
                    self.frame_queue.put(frame)
    
    def process_frames(self):
        """Thread de traitement des images"""
        while self.running:
            if not self.frame_queue.empty():
                frame = self.frame_queue.get()
                result = self.detect_features(frame)
                
                if not self.result_queue.full():
                    self.result_queue.put(result)
```

## Optimisations critiques

### Préprocessing intelligent
```python
def optimize_preprocessing(self, frame):
    """Préprocessing optimisé pour la détection oculaire"""
    # Conversion colorspace optimisée
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    # ROI (Region of Interest) pour réduire la zone de traitement
    height, width = gray.shape
    roi_y1, roi_y2 = height//4, 3*height//4  # Zone centrale
    roi_x1, roi_x2 = width//6, 5*width//6
    
    roi_gray = gray[roi_y1:roi_y2, roi_x1:roi_x2]
    
    # Égalisation d'histogramme pour améliorer le contraste
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    enhanced = clahe.apply(roi_gray)
    
    return enhanced, (roi_x1, roi_y1)
```

### Détection multi-échelle optimisée
```python
def detect_eyes_optimized(self, frame):
    """Détection d'yeux optimisée pour temps réel"""
    processed_frame, offset = self.optimize_preprocessing(frame)
    
    # Cascade classifier pré-entraîné
    eye_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_eye.xml'
    )
    
    # Paramètres optimisés pour vitesse/précision
    eyes = eye_cascade.detectMultiScale(
        processed_frame,
        scaleFactor=1.1,        # Réduction du facteur d'échelle
        minNeighbors=5,         # Équilibre détection/faux positifs
        minSize=(30, 30),       # Taille minimum des yeux
        maxSize=(150, 150),     # Taille maximum
        flags=cv2.CASCADE_SCALE_IMAGE
    )
    
    # Correction des coordonnées avec offset ROI
    corrected_eyes = []
    for (x, y, w, h) in eyes:
        corrected_eyes.append((
            x + offset[0], 
            y + offset[1], 
            w, h
        ))
    
    return corrected_eyes
```

## Techniques avancées

### Filtre de Kalman pour tracking
```python
import cv2
import numpy as np

class EyeTracker:
    def __init__(self):
        # Filtre de Kalman pour lisser le tracking
        self.kalman = cv2.KalmanFilter(4, 2)
        self.kalman.measurementMatrix = np.array([
            [1, 0, 0, 0],
            [0, 1, 0, 0]
        ], np.float32)
        
        self.kalman.transitionMatrix = np.array([
            [1, 0, 1, 0],
            [0, 1, 0, 1],
            [0, 0, 1, 0],
            [0, 0, 0, 1]
        ], np.float32)
        
        self.kalman.processNoiseCov = 0.03 * np.eye(4, dtype=np.float32)
        self.kalman.measurementNoiseCov = 0.1 * np.eye(2, dtype=np.float32)
        
    def update_tracking(self, detection):
        """Mise à jour du tracking avec nouvelle détection"""
        if detection is not None:
            # Prédiction
            prediction = self.kalman.predict()
            
            # Correction avec mesure
            measurement = np.array([[detection[0]], [detection[1]]], dtype=np.float32)
            self.kalman.correct(measurement)
            
            return prediction[:2].flatten()
        else:
            # Prédiction seule si pas de détection
            prediction = self.kalman.predict()
            return prediction[:2].flatten()
```

### Mesures précises des paramètres oculaires
```python
def measure_eye_parameters(self, eye_region):
    """Mesure précise des paramètres oculaires"""
    # Détection de la pupille par seuillage adaptatif
    gray_eye = cv2.cvtColor(eye_region, cv2.COLOR_BGR2GRAY)
    
    # Détection des contours de la pupille
    _, threshold = cv2.threshold(gray_eye, 0, 255, 
                                cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, 
                                   cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        # Sélection du plus grand contour (pupille)
        largest_contour = max(contours, key=cv2.contourArea)
        
        # Calcul des paramètres
        moments = cv2.moments(largest_contour)
        if moments["m00"] != 0:
            cx = int(moments["m10"] / moments["m00"])
            cy = int(moments["m01"] / moments["m00"])
            
            # Ellipse fitting pour précision
            if len(largest_contour) >= 5:
                ellipse = cv2.fitEllipse(largest_contour)
                center, axes, angle = ellipse
                
                return {
                    'center': (cx, cy),
                    'major_axis': max(axes),
                    'minor_axis': min(axes),
                    'angle': angle,
                    'area': cv2.contourArea(largest_contour)
                }
    
    return None
```

## Interface utilisateur temps réel

### Visualisation avec PyQt5
```python
import sys
from PyQt5.QtWidgets import QApplication, QLabel, QVBoxLayout, QWidget
from PyQt5.QtCore import QTimer, pyqtSignal
from PyQt5.QtGui import QImage, QPixmap
import cv2

class RealTimeVisionGUI(QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()
        self.vision_pipeline = RealTimeVisionPipeline()
        
        # Timer pour mise à jour de l'affichage
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_display)
        self.timer.start(33)  # ~30 FPS
        
    def initUI(self):
        self.setWindowTitle('Computer Vision Temps Réel')
        self.setGeometry(100, 100, 800, 600)
        
        layout = QVBoxLayout()
        
        # Label pour affichage video
        self.video_label = QLabel()
        layout.addWidget(self.video_label)
        
        # Label pour paramètres mesurés
        self.params_label = QLabel("Paramètres oculaires:")
        layout.addWidget(self.params_label)
        
        self.setLayout(layout)
        
    def update_display(self):
        """Mise à jour de l'affichage temps réel"""
        if not self.vision_pipeline.result_queue.empty():
            result = self.vision_pipeline.result_queue.get()
            
            # Conversion pour Qt
            rgb_image = cv2.cvtColor(result['frame'], cv2.COLOR_BGR2RGB)
            h, w, ch = rgb_image.shape
            bytes_per_line = ch * w
            
            qt_image = QImage(rgb_image.data, w, h, bytes_per_line, 
                            QImage.Format_RGB888)
            
            # Affichage
            pixmap = QPixmap.fromImage(qt_image)
            self.video_label.setPixmap(pixmap)
            
            # Mise à jour des paramètres
            if result['parameters']:
                params_text = f"""
                Centre pupille: {result['parameters']['center']}
                Axe majeur: {result['parameters']['major_axis']:.2f}px
                Axe mineur: {result['parameters']['minor_axis']:.2f}px
                Angle: {result['parameters']['angle']:.1f}°
                Surface: {result['parameters']['area']:.0f}px²
                """
                self.params_label.setText(params_text)
```

## Optimisations de performance

### Profiling et monitoring
```python
import cProfile
import time

class PerformanceMonitor:
    def __init__(self):
        self.frame_times = []
        self.processing_times = []
        
    def measure_frame_time(func):
        """Décorateur pour mesurer le temps de traitement"""
        def wrapper(self, *args, **kwargs):
            start_time = time.perf_counter()
            result = func(self, *args, **kwargs)
            end_time = time.perf_counter()
            
            processing_time = (end_time - start_time) * 1000  # ms
            self.processing_times.append(processing_time)
            
            # Garder seulement les 100 dernières mesures
            if len(self.processing_times) > 100:
                self.processing_times.pop(0)
                
            return result
        return wrapper
    
    def get_performance_stats(self):
        """Statistiques de performance"""
        if self.processing_times:
            avg_time = np.mean(self.processing_times)
            max_time = np.max(self.processing_times)
            fps = 1000 / avg_time if avg_time > 0 else 0
            
            return {
                'avg_processing_time': avg_time,
                'max_processing_time': max_time,
                'estimated_fps': fps,
                'frame_count': len(self.processing_times)
            }
        return None
```

## Conseils pratiques

### Gestion mémoire
- **Réutiliser les buffers** : Éviter les allocations répétées
- **Optimiser les ROI** : Traiter seulement les zones d'intérêt
- **Pool d'objets** : Réutiliser les structures de données

### Threading efficace
- **Séparer capture/traitement** : Thread dédié pour chaque tâche
- **Queues limitées** : Éviter l'accumulation de frames en retard
- **Synchronisation minimale** : Locks seulement si nécessaire

## Conclusion

Le développement d'applications de computer vision temps réel demande une approche rigoureuse alliant algorithmes efficaces, optimisations système et architecture logicielle adaptée. Mon expérience chez FittingBox m'a permis de maîtriser ces aspects cruciaux pour créer des solutions performantes et robustes.

Les techniques présentées dans cet article constituent une base solide pour tout développeur souhaitant se lancer dans la computer vision temps réel avec Python et OpenCV.

**Ressources utiles** :
- [Documentation OpenCV](https://docs.opencv.org/)
- [PyQt5 Documentation](https://doc.qt.io/qtforpython/)
- [NumPy Performance Guide](https://numpy.org/doc/stable/user/performance.html)